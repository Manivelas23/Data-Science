{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Clean Data with Python\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we receive raw data, we have to do a number of things before we’re ready to analyze it, possibly including:\n",
    "\n",
    "* **Diagnosing** the “tidiness” of the data — how much data cleaning we will have to do\n",
    "\n",
    "\n",
    "* **Reshaping** the data — getting right rows and columns for effective analysis\n",
    "\n",
    "\n",
    "* **Combining** multiple files\n",
    "\n",
    "\n",
    "* **Changing the types of values** — how we fix a column where numerical values are stored as strings, for example dropping or filling missing values - how we deal with data that is incomplete or missing\n",
    "\n",
    "\n",
    "* **Manipulating** strings to represent the data better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often describe data that is easy to analyze and visualize as “tidy data”. What does it mean to have tidy data?\n",
    "\n",
    "For data to be tidy, it must have:\n",
    "\n",
    "* Each variable as a separate column\n",
    "* Each row as a separate observation\n",
    "\n",
    "**The first step of diagnosing whether or not a dataset is tidy is using pandas functions to explore and probe the dataset.**\n",
    "\n",
    "We’ve seen most of the functions we often use to diagnose a dataset for cleaning. Some of the most useful ones are:\n",
    "\n",
    "* `.head()` — display the first 5 rows of the table\n",
    "\n",
    "\n",
    "* `.info()` — display a summary of the table\n",
    "\n",
    "\n",
    "* `.describe()` — display the summary statistics of the table\n",
    "\n",
    "\n",
    "* `.columns` — display the column names of the table\n",
    "\n",
    "\n",
    "* `.value_counts()` — display the distinct values for a column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say that we have a ton of files following the filename structure: `'file1.csv'`, `'file2.csv'`, `'file3.csv'`, and so on. \n",
    "\n",
    "We can combine the use of `glob`, a Python library for working with files, with pandas to organize this data better. `glob` can open multiple files by using regex matching to get the filenames:\n",
    "```python\n",
    "import glob\n",
    "files = glob.glob(\"file*.csv\")\n",
    "df_list = []\n",
    "for filename in files:\n",
    "  data = pd.read_csv(filename)\n",
    "  df_list.append(data)\n",
    "df = pd.concat(df_list)\n",
    "print(files) \n",
    "```\n",
    "\n",
    "This code goes through any file that starts with 'file' and has an extension of `.csv`. It opens each file, reads the data into a DataFrame, and then concatenates all of those DataFrames together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want\n",
    "\n",
    "* **Each variable as a separate column**\n",
    "* **Each row as a separate observation**\n",
    "\n",
    "\n",
    "We would want to reshape a table like:\n",
    "\n",
    "| Account    | Checking | Savings |   |\n",
    "|------------|----------|---------|---|\n",
    "| “12456543” | 8500     | 8900    |   |\n",
    "| “12283942” | 6410     | 8020    |   |\n",
    "| “12839485” | 78000    | 92000   |   |\n",
    "|            |          |         |   |\n",
    "\t\n",
    "\n",
    "Into a table that looks more like:\n",
    "\n",
    "| Account    | Account Type | Amount |   |\n",
    "|------------|--------------|--------|---|\n",
    "| “12456543” | “Checking”   | 8500   |   |\n",
    "| “12456543” | “Savings”    | 8900   |   |\n",
    "| \"12283942\" | “Checking”   | 6410   |   |\n",
    "| \"12283942” | “Savings”    | 8020   |   |\n",
    "\n",
    "We can use `pd.melt()` to do this transformation. `.melt()` takes in a DataFrame, and the columns to unpack:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "pd.melt(frame=df, \n",
    "   id_vars='name', \n",
    "   value_vars=['Checking','Savings'], \n",
    "   value_name=\"Amount\", \n",
    "   var_name=\"Account Type\")\n",
    "```\n",
    "        \n",
    "        \n",
    "        \n",
    "The parameters you provide are:\n",
    "\n",
    "* **frame**: the DataFrame you want to melt\n",
    "* **id_vars**: the column(s) of the old DataFrame to preserve\n",
    "* **value_vars**: the column(s) of the old DataFrame that you want to turn into variables\n",
    "* **value_name**: what to call the column of the new DataFrame that stores the values\n",
    "* **var_name**: what to call the column of the new DataFrame that stores the variables\n",
    "\n",
    "\n",
    "The default names may work in certain situations, but it’s best to always have data that is self-explanatory. Thus, we often use `.columns()` to rename the columns after melting:\n",
    "\n",
    "`df.columns([\"Account\", \"Account Type\", \"Amount\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we see duplicated rows of data in the DataFrames we are working with. This could happen due to errors in data collection or in saving and loading the data.\n",
    "\n",
    "To check for duplicates, we can use the pandas function `.duplicated()`, which will **return a Series telling us which rows are duplicate rows**.\n",
    "\n",
    "Let’s say we have a DataFrame fruits that represents this table:\n",
    "\n",
    "| item         | price   | calories |\n",
    "|--------------|---------|----------|\n",
    "| “banana”     | “\\\\$1”  | 105      |\n",
    "| “apple”      | “\\\\$0.75” | 95       |\n",
    "| \"apple\"      | “\\\\$0.75” | 95       |\n",
    "| \"peach”      | “\\\\$3”    | 55       |\n",
    "| \"peach\"      | “\\\\$4”    | 55       |\n",
    "| “clementine” | “\\\\$2.5”  | 35       |\n",
    "\n",
    "\n",
    "If we call `fruits.duplicated()`, we would get the following table:\n",
    "\n",
    "| id | value |\n",
    "|----|-------|\n",
    "| 0  | False |\n",
    "| 1  | False |\n",
    "| 2  | True  |\n",
    "| 3  | False |\n",
    "| 4  | False |\n",
    "| 5  | False |\n",
    "\n",
    "We can see that row 2, which represents an \"apple\" with price \"\\\\$0.75\" and 95 calories, is a duplicate row. Every value in this row is the same as in another row.\n",
    "\n",
    "We can use the pandas `.drop_duplicates()` function to remove all rows that are duplicates of another row.\n",
    "\n",
    "If we call `fruits.drop_duplicates()`, we would get the table:\n",
    "\n",
    "| item         | price   | calories |\n",
    "|--------------|---------|----------|\n",
    "| “banana”     | “\\\\$1”  | 105      |\n",
    "| “apple”      | “\\\\$0.75” | 95       |\n",
    "| \"peach”      | “\\\\$3”    | 55       |\n",
    "| \"peach\"      | “\\\\$4”    | 55       |\n",
    "| “clementine” | “\\\\$2.5”  | 35       |\n",
    "\n",
    "The \"apple\" row was deleted because it was exactly the same as another row. But the two \"peach\" rows remain because there is a difference in the price column.\n",
    "\n",
    "If we wanted to **remove every row with a duplicate value in the item column, we could specify a subset**:\n",
    "\n",
    "```python\n",
    "fruits = fruits.drop_duplicates(subset=['item'])\n",
    "```\n",
    "\n",
    "By default, this keeps the first occurrence of the duplicate:\n",
    "\n",
    "| item         | price   | calories |\n",
    "|--------------|---------|----------|\n",
    "| “banana”     | “\\\\$1”  | 105      |\n",
    "| “apple”      | “\\\\$0.75” | 95       |\n",
    "| \"peach”      | “\\\\$3”    | 55       |\n",
    "| “clementine” | “\\\\$2.5”  | 35       |\n",
    "\n",
    "\n",
    "Make sure that the columns you drop duplicates from are specifically the ones where duplicates don’t belong. You wouldn’t want to drop duplicates with the price column as a subset, for example, because it’s okay if multiple items cost the same amount!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting by Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In trying to get clean data, we want to make sure each column represents one type of measurement. Often, multiple measurements are recorded in the same column, and we want to separate these out so that we can do individual analysis on each variable.\n",
    "\n",
    "Let’s say we have a column “birthday” with data formatted in **MMDDYYYY** format. In other words, **“11011993”** represents a birthday of November 1, 1993. We want to split this data into day, month, and year so that we can use these columns as separate features.\n",
    "\n",
    "In this case, we know the exact structure of these strings. The first two characters will always correspond to the month, the second two to the day, and the rest of the string will always correspond to year. We can easily break the data into three separate columns by splitting the strings using `.str`:\n",
    "\n",
    "\n",
    "```python\n",
    "# Create the 'month' column\n",
    "df['month'] = df.birthday.str[0:2]\n",
    " \n",
    "# Create the 'day' column\n",
    "df['day'] = df.birthday.str[2:4]\n",
    " \n",
    "# Create the 'year' column\n",
    "df['year'] = df.birthday.str[4:]\n",
    "```\n",
    "\n",
    "The first command takes the first two characters of each value in the birthday column and puts it into a month column. The second command takes the second two characters of each value in the birthday column and puts it into a day column. The third command takes the rest of each value in the birthday column and puts it into a year column.\n",
    "\n",
    "This would transform a table like:\n",
    "\n",
    "| id   | birthday   |\n",
    "|------|------------|\n",
    "| 1011 | “12241989” |\n",
    "| 1112 | “10311966” |\n",
    "| 1113 | “01052011” |\n",
    "\n",
    "\n",
    "into a table like:\n",
    "\n",
    "| id   | birthday   | day | month | year |\n",
    "|------|------------|-----|-------|------|\n",
    "| 1011 | “12241989” | 24  | 12    | 1989 |\n",
    "| 1112 | “10311966” | 31  | 10    | 1966 |\n",
    "| 1113 | “01052011” | 05  | 01    | 2011 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting by Character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we have a column called “type” with data entries in the format **\"admin_US\"** or **\"user_Kenya\"**. Just like we saw before, this column actually contains two types of data. One seems to be the user type (with values like “admin” or “user”) and one seems to be the country this user is in (with values like “US” or “Kenya”).\n",
    "\n",
    "We can no longer just split along the first 4 characters because admin and user are of different lengths. Instead, we know that we want to **split along the \"_\"**. Using that, we can split this column into two separate, cleaner columns:\n",
    "\n",
    "```python\n",
    "# Create the 'str_split' column\n",
    "df['str_split'] = df.type.str.split('_')\n",
    " \n",
    "# Create the 'usertype' column\n",
    "df['usertype'] = df.str_split.str.get(0)\n",
    " \n",
    "# Create the 'country' column\n",
    "df['country'] = df.str_split.str.get(1)\n",
    "```\n",
    "\n",
    "This would transform a table like:\n",
    "\n",
    "|id\t|type|\n",
    "|---|-----|\n",
    "|1011|\t“user_Kenya”|\n",
    "|1112|\t“admin_US”|\n",
    "|1113|\t“moderator_UK”|\n",
    "\n",
    "into a table like:\n",
    "\n",
    "|id\t|type\t|country\t|usertype|\n",
    "|---|--------|----------|---------|\n",
    "1011|“user_Kenya”|   “Kenya”|“user”|\n",
    "1112|“admin_US”\t|    “US”|“admin”|\n",
    "1113|“moderator_UK”| “UK”|“moderator”|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of a DataFrame can hold items of the same data type or dtype. The dtypes that pandas uses are: `float, int, bool, datetime, timedelta, category and object`. Often, we want to convert between types so that we can do better analysis. If a numerical category like \"num_users\" is stored as a Series of objects instead of ints, for example, it makes it more difficult to do something like make a line graph of users over time.\n",
    "\n",
    "To see the types of each column of a DataFrame, we can use:\n",
    "\n",
    "`print(df.dtypes)`\n",
    "\n",
    "For a DataFrame like this:\n",
    "\n",
    "|item|\tprice|\tcalories|\n",
    "|----|--------|---------|\n",
    "|“banana”|\t“\\\\$1”|\t105|\n",
    "|“apple”|\t“\\\\$0.75”|\t95|\n",
    "|“peach”|\t“\\\\$3”|\t55|\n",
    "|“clementine”|\t“\\\\$2.5”|\t35|\n",
    "\n",
    "the `.dtypes` attribute would be:\n",
    "\n",
    "|item|object|\n",
    "|-----|------|\n",
    "|price     |  object|\n",
    "|calories  |   int64|\n",
    "|dtype: object|\n",
    "\n",
    "We can see that the dtype of the dtypes attribute itself is an object! It is a Series object, which you have already worked with. Series objects compose all DataFrames.\n",
    "\n",
    "We can see that the price column is made up of objects, which will probably make our analysis of price more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need to modify strings in our DataFrames to help us transform them into more meaningful metrics. For example, in our fruits table from before:\n",
    "\n",
    "| item         | price   | calories |\n",
    "|--------------|---------|----------|\n",
    "| “banana”     | “\\\\$1”  | 105      |\n",
    "| “apple”      | “\\\\$0.75” | 95       |\n",
    "| \"peach”      | “\\\\$3”    | 55       |\n",
    "| “clementine” | “\\\\$2.5”  | 35       |\n",
    "\n",
    "\n",
    "We can see that the `'price'` column is actually composed of strings representing dollar amounts. This column could be much better represented in floats, so that we could take the mean, calculate other aggregate statistics, or compare different fruits to one another in terms of price.\n",
    "\n",
    "First, we can use what we know of regex to get rid of all of the dollar signs:\n",
    "\n",
    "```python\n",
    "fruit.price = fruit['price'].replace('[\\$,]', '', regex=True)\n",
    "```\n",
    "\n",
    "Then, we can use the pandas function `.to_numeric()` to convert strings containing numerical values to integers or floats:\n",
    "\n",
    "```python\n",
    "fruit.price = pd.to_numeric(fruit.price)\n",
    "```\n",
    "Now, we have a DataFrame that looks like:\n",
    "\n",
    "| item         | price   | calories |\n",
    "|--------------|---------|----------|\n",
    "| “banana”     | 1        | 105      |\n",
    "| “apple”      | 0.75 | 95       |\n",
    "| \"peach”      | 3   | 55       |\n",
    "| “clementine” | 2.5  | 35       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More String Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to do analysis on **numbers that are hidden within string values**. We can use regex to extract this numerical data from the strings they are trapped in. Suppose we had this DataFrame df representing a workout regimen:\n",
    "\n",
    "|date\t|exerciseDescription|\n",
    "|---|-----|\n",
    "|10/18/2018\t|“lunges - 30 reps”|\n",
    "|10/18/2018\t|“squats - 20 reps”|\n",
    "|10/18/2018\t|“deadlifts - 25 reps”|\n",
    "|10/18/2018\t|“jumping jacks - 30 reps”\n",
    "|10/19/2018\t|“lunges - 40 reps”|\n",
    "|10/19/2018\t|“chest flyes - 15 reps”|\n",
    "\n",
    "It would be helpful to separate out data like \"30 lunges\" into 2 columns with the number of reps, \"30\", and the type of exercise, \"lunges\". Then, we could compare the increase in the number of lunges done over time, for example.\n",
    "\n",
    "To extract the numbers from the string we can use pandas’ .str.split() function:\n",
    "```python\n",
    "split_df = df['exerciseDescription'].str.split('(\\d+)', expand=True)\n",
    "```\n",
    "which would result in this DataFrame `split_df`:\n",
    "\n",
    "|* *\t|0\t|1\t|2|\n",
    "|-----|------|-----|-----|\n",
    "|0\t|“lunges - “|\t“30”|\t“reps”|\n",
    "|1\t|“squats - “|\t“20”|\t“reps”|\n",
    "|2\t|“deadlifts - “|\t“25”|\t“reps”|\n",
    "|3\t|“jumping jacks - “|\t“30”|\t“reps”|\n",
    "|4\t|“lunges - “\t|“40”\t|“reps”|\n",
    "|5\t|“chest flyes - “|\t“15”|\t“reps”|\n",
    "\n",
    "\n",
    "Then, we can assign columns from this DataFrame to the original df:\n",
    "\n",
    "```python\n",
    "df.reps = pd.to_numeric(split_df[1])\n",
    "df.exercise = split_df[2].replace('[\\- ]', '', regex=True)\n",
    "```\n",
    "\n",
    "Now, our df looks like this:\n",
    "\n",
    "|date|\texerciseDescription|\treps\t|exercise|\n",
    "|---|---|---|---|\n",
    "|10/18/2018\t|“lunges - 30 reps”\t|30\t|“lunges”|\n",
    "|10/18/2018\t|“squats - 20 reps”|\t20\t|“squats”\n",
    "|10/18/2018\t|“deadlifts - 25 reps”\t|25\t|“deadlifts”|\n",
    "|10/18/2018\t|“jumping jacks - 30 reps”\t|30\t|“jumping jacks”|\n",
    "|10/19/2018\t|“lunges - 40 reps”\t|40\t|“lunges”|\n",
    "|10/19/2018\t|“chest flyes - 15 reps”\t|15\t|“chest flyes”|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values\n",
    "We often have data with missing elements, as a result of a problem with the data collection process or errors in the way the data was stored. The missing elements normally show up as NaN (or Not a Number) values:\n",
    "\n",
    "|day|\tbill|\ttip|\tnum_guests|\n",
    "|----|----|----|----|\n",
    "|“Mon”|\t10.1|\t1|\t1|\n",
    "|“Mon”|\t20.75|\t5.5|\t2|\n",
    "|“Tue”|\t19.95|\t5.5|\tNaN|\n",
    "|“Wed”|\t44.10|\t15|\t3|\n",
    "|“Wed”|\tNaN\t|1|\t1|\n",
    "\n",
    "The `num_guests` value for the 3rd row is missing, and the `bill` value for the 5th row is missing. Some calculations we do will just skip the NaN values, but some calculations or visualizations we try to perform will break when a NaN is encountered.\n",
    "\n",
    "Most of the time, we use one of two methods to deal with missing values.\n",
    "\n",
    "### Method 1: drop all of the rows with a missing value\n",
    "We can use `.dropna()` to do this:\n",
    "\n",
    "\n",
    "`bill_df = bill_df.dropna()`\n",
    "\n",
    "\n",
    "This command will result in the DataFrame without the incomplete rows:\n",
    "\n",
    "|day|\tbill|\ttip|\tnum_guests|\n",
    "|---|---|---|---|\n",
    "|“Mon”|\t10.1|\t1|\t1|\n",
    "|“Mon”|\t20.75|\t5.5|\t2|\n",
    "|“Wed”|\t44.10|\t15|\t3|\n",
    "\n",
    "If we wanted to remove every row with a `NaN` value in the `num_guests` column only, we could specify a subset:\n",
    "\n",
    "`bill_df = bill_df.dropna(subset=['num_guests'])`\n",
    "\n",
    "\n",
    "### Method 2: fill the missing values with the **mean** of the column, or with some other aggregate value.\n",
    "We can use `.fillna()` to do this:\n",
    "\n",
    "```python\n",
    "bill_df = bill_df.fillna(value=\n",
    "            {\n",
    "             \"bill\":bill_df.bill.mean(), \n",
    "             \"num_guests\":bill_df.num_guests.mean()\n",
    "            })\n",
    "```\n",
    "This command will result in the DataFrame with the respective mean of the column in the place of the original NaNs:\n",
    "\n",
    "|day  | bill\t|tip|\tnum_guests\n",
    "|----|----|----|----|\n",
    "|“Mon”|\t10.1\t|1\t|1\n",
    "|“Mon”|\t20.75\t|5.5|\t2\n",
    "|“Tue”|\t19.95\t|5.5|\t1.75\n",
    "|“Wed”|\t44.10\t|15\t|3\n",
    "|“Wed”|\t23.725\t|1\t|1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
