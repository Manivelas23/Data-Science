{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invalid-nutrition",
   "metadata": {},
   "source": [
    "# Data Centering and Scaling\n",
    "## An introduction to data centering, min-max normalization, and standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-commons",
   "metadata": {},
   "source": [
    "In this article, you will learn how to center and scale your data using common techniques. You will learn:\n",
    "\n",
    "* How to center data and interpret centered data\n",
    "* Why scaling your data is important\n",
    "* How to scale data using two common methods:\n",
    "    * Min-max Normalization\n",
    "    * Standardization  \n",
    "* When to choose normalization vs. standardization\n",
    "Let’s get started!\n",
    "\n",
    "### Data Centering\n",
    "\n",
    "Data centering involves subtracting the mean of a data set from each data point so that the new mean is 0. Mathematically, this looks like:\n",
    "\n",
    "\n",
    "$$\n",
    "Xcentered_i = X_i - \\mu\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where `X_i` is a datapoint and the Greek letter `μ` is the mean of all the `X` values.\n",
    "\n",
    "For example, let’s take a look at a data set of ages for five individuals:\n",
    "```python\n",
    "ages = [24, 40, 28, 22, 56]\n",
    "```\n",
    "\n",
    "\n",
    "The mean age in this data set is **`34 years old`**.\n",
    "\n",
    "To center our data, we subtract the mean from each data point in ages:\n",
    "\n",
    "```python\n",
    "centered_ages = [-10, 6, -6, -12, 22]\n",
    "```\n",
    "\n",
    "This centered data is useful because it tells us how far above or below the mean each data point is, giving us additional insight that we can’t get just by looking at the initial data set. For example, the age of the first individual is ten years below the average.\n",
    "\n",
    "Note that, because the **sum of the centered values is 0 (-10 + 6 - 6 - 12 + 22 = 0), the mean of the centered data is 0.**\n",
    "\n",
    "\n",
    "### Data Scaling\n",
    "\n",
    "A common task for data analysts and scientists is to find trends in data by comparing features of data points. However, this task is made difficult when the features are on drastically different scales.\n",
    "\n",
    "For instance, let’s consider a data set containing two features, **`age`** and **`income`**.\n",
    "\n",
    "In general, a person’s age usually ranges from 0 to about 100 years. A person’s income, on the other hand, usually ranges from 0 to large amounts measured in the thousands of dollars. Clearly, age and income are two features that have vastly different ranges.\n",
    "\n",
    "<img src=https://static-assets.codecademy.com/Paths/data-analyst-career-path/center-scaling/scaling.png width=1000>\n",
    "\n",
    "This presents issues when trying to use many machine learning algorithms, which treat all dimensions equally regardless of their scale. The difference in one year of age is interpreted as exactly equal to the difference in one dollar of income. That makes no sense!\n",
    "\n",
    "In other words, the income feature outweighs the importance of age because income is on a relatively huge scale. Take a look at the following image:\n",
    "\n",
    "<img src=https://static-assets.codecademy.com/Paths/data-analyst-career-path/center-scaling/unnormalized.png width=600>\n",
    "\n",
    "\n",
    "From this chart, it is impossible to notice any relationship between income and age – all of the data is squished to the left. This is because the feature with the larger scale (income) dominates the smaller feature (age).\n",
    "\n",
    "We would like every datapoint to have the same scale so each feature contributes equally to the relationship. Data scaling lets us achieve this.\n",
    "\n",
    "Two of the most commonly used data scaling techniques are:\n",
    "\n",
    "* Min-max normalization\n",
    "* Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5af2f",
   "metadata": {},
   "source": [
    "***\n",
    "### Min-Max Normalization\n",
    "\n",
    "Min-max normalization is one of the most simple and common ways to scale data.\n",
    "\n",
    "For every feature in a data set, the minimum value of that feature is transformed into 0, the maximum value is transformed into 1, and every other value is transformed into a decimal between 0 and 1.\n",
    "\n",
    "For example, if the minimum value of a feature is 10, and the maximum value is 30, then 20 would be transformed to 0.5 since it is halfway between 10 and 30.\n",
    "\n",
    "The formula for min-max normalization is as follows:\n",
    "\n",
    "$$\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "Using min-max normalization, our previous chart of income vs. age looks like the following:\n",
    "\n",
    "<img src=https://static-assets.codecademy.com/Paths/data-analyst-career-path/center-scaling/normalized.png width=600>\n",
    "\n",
    "Notice that the data is now properly scaled – both age and income fall in the range of [0,1].\n",
    "\n",
    "Let’s get some practice implementing min-max normalization in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d093bdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.25, 0.5, 0.75, 0.1]\n",
      "[0.0, 0.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(lst):\n",
    "  minimum = min(lst)\n",
    "  maximum = max(lst)\n",
    "  normalized = []\n",
    "\n",
    "  for i in lst:\n",
    "      if lst[0] ==0: \n",
    "        normalized.append(i/100)\n",
    "      else:\n",
    "        normalized.append((i - minimum) / (maximum-minimum))\n",
    "  return normalized\n",
    "\n",
    "# Uncomment these function calls to test your function:\n",
    "print(min_max_normalize([0, 25, 50, 75, 10]))\n",
    "# should print [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "print(min_max_normalize([10, 12, 14]))\n",
    "# should print [0.0, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a61429",
   "metadata": {},
   "source": [
    "One downside of min-max normalization is that it does not handle outliers very well. For example, if you have 99 values between 0 and 20, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.2 while the outlier is transformed to 1. This results in skewed data:\n",
    "\n",
    "\n",
    "<img src=https://static-assets.codecademy.com/Paths/data-analyst-career-path/center-scaling/outlier.png width=600>\n",
    "\n",
    "\n",
    "In this example, normalizing the data fixed the skewing problem on the x-axis, but now the y-axis is causing problems.\n",
    "\n",
    "As we’ll see shortly, standardization is a more robust data scaling method for dealing with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e88f54",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization (also known as Z-score normalization) is another common data scaling technique.\n",
    "\n",
    "Standardization involves subtracting the mean of each observation and then dividing by the standard deviation:\n",
    "\n",
    "$$\n",
    "z = \\frac{value - mean}{{stdev}}\n",
    "$$\n",
    "\n",
    "Once standardization is complete, all the features will have a mean of zero, a standard deviation of one, and therefore, the same scale.\n",
    "\n",
    "Unlike normalization, standardization does not have a bounding range. This means that even if you have outliers in your data, your standardized data will not be affected. Therefore, if your dataset has outliers, standardization is the preferred scaling technique.\n",
    "\n",
    "Let’s see if you can use the formula above to implement standardization in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd37b77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4184397163120568, -0.7092198581560284, 0.0, 0.7092198581560284, 1.4184397163120568]\n",
      "[-1.2254901960784315, 0.0, 1.2254901960784315]\n"
     ]
    }
   ],
   "source": [
    "def standardize(lst, mean, std_dev):\n",
    "  standardized = [(i-mean) / std_dev for i in lst]\n",
    "  return standardized\n",
    "\n",
    "# Uncomment these function calls to test your standardize function:\n",
    "print(standardize([1, 2, 3, 4, 5], 3.0, 1.41))\n",
    "# should print [-1.418, -0.709, 0.0, 0.709, 1.418]\n",
    "print(standardize([10, 15, 20], 15.0, 4.08))\n",
    "# should print [-1.225, 0.0, 1.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d3337",
   "metadata": {},
   "source": [
    "***\n",
    "### When to Normalize vs. Standardize?\n",
    "\n",
    "Min-max normalization and standardization both have a similar goal of transforming features in data to have the same scale so that each feature is equally important. So when should you use min-max normalization vs. standardization?\n",
    "\n",
    "There is not always a clear answer. Both normalization and standardization have their strengths as well as their drawbacks. For example, if you need your data to be on a 0-1 scale, then it makes sense to use min-max normalization. If you have outliers in your data, then it is best to use standardization (Z-score normalization) since it does not have a bounding range like min-max normalization does.\n",
    "\n",
    "Keep in mind that not every data set requires normalization or standardization. If your data features do not have vastly different ranges, then scaling your data might not be necessary.\n",
    "\n",
    "### Python Implementation\n",
    "\n",
    "As you saw, it is possible to implement min-max normalization and standardization by writing your own Python functions. However, in practice, most data analysts and scientists use popular libraries such as scikit-learn, which makes it very easy to scale your data.\n",
    "\n",
    "For example, to normalize your data you can import **`MinMaxScaler`** from the **`sklearn.preprocessing`** package and then make a simple function call:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    " \n",
    "# read in data \n",
    "data = pd.read_csv('data.csv')\n",
    " \n",
    "# normalize data \n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "Likewise, standardizing your data is easy to do in just a few lines of code:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    " \n",
    "# read in data \n",
    "data = pd.read_csv('data.csv')\n",
    " \n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "Later on, you’ll get practice with transforming your data in Python using real-world data. For now, let’s test your knowledge of min-max normalization and standardization with a few questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad66d5d",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "As you learned in this article, normalization and standardization are scaling techniques that are slightly different but have a similar motive: to put data features in the same scale so that no feature is dominated by the other. These techniques are widely used by data analysts and scientists.\n",
    "\n",
    "In this article, you learned:\n",
    "\n",
    "### Data centering:\n",
    "\n",
    "* Data centering involves subtracting the mean of a data set from each data point so that the new mean is 0.\n",
    "* Centered data is useful because it tells us how far above or below the mean each data point is.\n",
    "\n",
    "\n",
    "### Min-max normalization:\n",
    "\n",
    "* The goal of normalization is to put features with different ranges onto the same scale.\n",
    "* For every feature in a data set, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.\n",
    "* A downside of normalization is that it does not handle outliers well\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Standardization:\n",
    "\n",
    "* Standardization, also known as Z-score normalization, involves subtracting the mean of each observation and then dividing by the standard deviation:\n",
    "\n",
    "$$\n",
    "z = \\frac{value - mean}{{stdev}}\n",
    "$$\n",
    "\n",
    "* Once standardization is complete, all standardized features will have a mean of zero, a standard deviation of one, and therefore, the same scale.\n",
    "* Unlike normalization, standardization does not have a bounding range. This means standardization can deal with outliers.\n",
    "\n",
    "\n",
    "As a data analyst or scientist, it’s important to understand the data you’re working with and how to transform that data. You now have a better grasp of common data transformation techniques and why it’s important to transform your data in the first place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
